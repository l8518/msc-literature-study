{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = psycopg2.connect(\n",
    "    user=\"lvs215\",\n",
    "    password=\"\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    database=\"lvs215\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        \"name\": \"reproducibility\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title \n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%') \n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-system\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title \n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%') \n",
    "                AND\n",
    "                (lower(ps.title) LIKE '%system%' OR lower(ps.abstract) LIKE '%system%') \n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-systems\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title \n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%') \n",
    "                AND\n",
    "                (lower(ps.title) LIKE '%systems%' OR lower(ps.abstract) LIKE '%systems%') \n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-or-repeatability\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations\n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%')\n",
    "                    OR\n",
    "                    (lower(ps.title) LIKE '%repeatability%' OR lower(ps.abstract) LIKE '%repeatability%')\n",
    "                )\n",
    "                ORDER BY ps.id\n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-or-repeatability-or-replicability\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations\n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%')\n",
    "                    OR\n",
    "                    (lower(ps.title) LIKE '%repeatability%' OR lower(ps.abstract) LIKE '%repeatability%')\n",
    "                    OR\n",
    "                    (lower(ps.title) LIKE '%replicability%' OR lower(ps.abstract) LIKE '%replicability%')\n",
    "                )\n",
    "                ORDER BY ps.id\n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-or-repeatability-or-replicability-v2\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations, string_agg(a.name, ', ') as authors\n",
    "                FROM publications ps \n",
    "                    LEFT JOIN author_paper_pairs app ON CAST(ps.id AS VARCHAR) = app.paper_id\n",
    "                    LEFT JOIN authors a ON CAST(a.id AS VARCHAR) = app.author_id\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) ~ 'reproducibility' OR lower(ps.abstract) ~ 'reproducibility')\n",
    "                    OR\n",
    "                    (lower(ps.title) ~ 'repeatability' OR lower(ps.abstract) ~ 'repeatability')\n",
    "                    OR\n",
    "                    (lower(ps.title) ~ 'replicability' OR lower(ps.abstract) ~ 'replicability')\n",
    "                )\n",
    "                GROUP BY ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations\n",
    "                ORDER BY ps.id\n",
    "                \"\"\",\n",
    "        \"run\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"repeatability-replicability-reproducibility-corroborability-all\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations, string_agg(a.name, ', ') as authors\n",
    "                FROM publications ps \n",
    "                    LEFT JOIN author_paper_pairs app ON CAST(ps.id AS VARCHAR) = app.paper_id\n",
    "                    LEFT JOIN authors a ON CAST(a.id AS VARCHAR) = app.author_id\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) ~ 'repeatability|repeat|repeatability' OR lower(ps.abstract) ~ 'repeatability|repeat|repeatability')\n",
    "                    OR\n",
    "                    (lower(ps.title) ~ 'replicability|replicate|replication' OR lower(ps.abstract) ~ 'replicability|replicate|replication')\n",
    "                    OR\n",
    "                    (lower(ps.title) ~ 'reproducibility|reproduce|reproduction' OR lower(ps.abstract) ~ 'reproducibility|reproduce|reproduction')\n",
    "                    OR\n",
    "                    (lower(ps.title) ~ 'corroborability|corroborate|Corroboration' OR lower(ps.abstract) ~ 'corroborability|corroborate|Corroboration')\n",
    "                )\n",
    "                GROUP BY ps.id, ps.title, ps.abstract, ps.year, ps.venue, ps.doi, ps.n_citations\n",
    "                ORDER BY ps.id\n",
    "                \"\"\",\n",
    "        \"run\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-or-repeatability-system\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title \n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%')\n",
    "                    OR\n",
    "                    (lower(ps.title) LIKE '%repeatability%' OR lower(ps.abstract) LIKE '%repeatability%')\n",
    "                )\n",
    "                AND\n",
    "                (lower(ps.title) LIKE '%system%' OR lower(ps.abstract) LIKE '%system%') \n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reproducibility-or-repeatability-systems\",\n",
    "        \"query\": \"\"\"\n",
    "                SELECT ps.id, ps.title \n",
    "                FROM publications ps\n",
    "                WHERE \n",
    "                ( \n",
    "                    (lower(ps.title) LIKE '%reproducibility%' OR lower(ps.abstract) LIKE '%reproducibility%')\n",
    "                    OR\n",
    "                    (lower(ps.title) LIKE '%repeatability%' OR lower(ps.abstract) LIKE '%repeatability%')\n",
    "                )\n",
    "                AND\n",
    "                (lower(ps.title) LIKE '%systems%' OR lower(ps.abstract) LIKE '%systems%') \n",
    "                \"\"\",\n",
    "        \"run\": False\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query(query_dict):\n",
    "    print(query_dict['name'])\n",
    "    query_results = []\n",
    "    if not query_dict['run']:\n",
    "        print('skipped')\n",
    "        return\n",
    "    with db.cursor() as cursor:\n",
    "        cursor.execute(query_dict[\"query\"])\n",
    "        field_names = [field[0] for field in cursor.description]\n",
    "        query_results = cursor.fetchall()\n",
    "    print(\"General Statistics:\", \"\\n\", \"result length\", len(query_results), \"\\n\")\n",
    "    with open(f\"{query_dict['name']}.csv\", 'w+', newline ='') as file:\n",
    "        write = csv.writer(file)\n",
    "        write.writerow(field_names)\n",
    "        write.writerows(query_results)\n",
    "    for pub in query_results[1:10]:\n",
    "        print(pub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################ \n",
      "\n",
      "reproducibility\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-system\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-systems\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-or-repeatability\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-or-repeatability-or-replicability\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-or-repeatability-or-replicability-v2\n",
      "General Statistics: \n",
      " result length 151 \n",
      "\n",
      "('13a572f4922a195882e6e1a1ddc5c2fabddde7c8', 'Research Software Discovery: An Overview', 'Research software is an integral part of scientific investigations. The paper identifies challenges, risks and new opportunities in research software publication and discovery. The diverse code discovery landscape is mapped and agents with their business models identified. Examples for discovery tools and strategies are given to support the classification. Reproducibility of research and reuse of code may improve if software discovery was easier. Researchers conducting a search for existing software in the context of a state-of-the-art report or a software management plan could use this paper as a guideline for their information retrieval strategy.', 2018, 'e-Science', '10.1109/eScience.2018.00016', 0, None)\n",
      "('15ae1db945d59551d403e1f3b7f3f2305a2f5d7e', 'PRUNE: A preserving run environment for reproducible scientific computing', 'Computing as a whole suffers from a crisis of reproducibility. Programs executed in one context are astonishingly hard to reproduce in another context, resulting in wasted effort by people and general distrust of results produced by computer. The root of the problem lies in the fact that every program has implicit dependencies on data and execution environment which are rarely understood by the end user. To address this problem, we present PRUNE, the Preserving Run Environment. In PRUNE, every task to be executed is wrapped in a functional interface and coupled with a strictly defined environment. The task is then executed by PRUNE rather than the user to ensure reproducibility. As a scientific workflow evolves in PRUNE, a growing but immutable tree of derived data is created. The provenance of every item in the system can be precisely described, facilitating sharing and modification between collaborating researchers, along with efficient management of limited storage space. We present the user interface and the initial prototype of PRUNE, and demonstrate its application in matching records and comparing surnames in U.S. Censuses.', 2016, 'e-Science', '10.1109/eScience.2016.7870886', 1, None)\n",
      "('22e60ddc66df7ee6dde87cea20716dcf7cef1522', 'Research and Realization of Kth Path Planning Algorithm under Large-Scale Data Which Meets the Requirement for Repeatability', 'Path planning problem is one of the key contents in the research field of Geographic Information System (GIS), among which the finding of the shortest path is always a hot topic. When there is large quantity of data, the efficiency of traditional algorithms of Kth shortest paths is relatively low, and some path problems concerning big difference between the planned Kth paths under actual requirements cannot be solved. On the basis of Dijkstra algorithm, the concepts of favorability and repeatability are introduced to find Kth paths including the shortest in the current graph cyclically by determination of repeatability of path results and the change of graph caused by the change of favorability, thereby realizing the planning of multiple different paths. Compared with similar algorithms, this algorithm is faster, and the multiple path results obtained meet the requirement for certain repeatability while the lengths of paths are also quite reasonable.', 2014, 'ISCID', '10.1109/ISCID.2014.148', 0, None)\n",
      "('2f431e669801afb58e5624c89026dff5b94663eb', 'Accelerating data-driven discovery with scientific asset management', \"The overhead and burden of managing data in complex discovery processes involving experimental protocols with numerous data-producing and computational steps has become the gating factor that determines the pace of discovery. The lack of comprehensive systems to capture, manage, organize and retrieve data throughout the discovery life cycle leads to significant overheads on scientists' time and effort, reduced productivity, lack of reproducibility, and an absence of data sharing. In â\\x80\\x9ccreative fieldsâ\\x80\\x9d like digital photography and music, digital asset management (DAM) systems for capturing, managing, curating and consuming digital assets like photos and audio recordings, have fundamentally transformed how these data are used. While asset management has not taken hold in eScience applications, we believe that transformation similar to that observed in the creative space could be achieved in scientific domains if appropriate ecosystems of asset management tools existed to capture, manage, and curate data throughout the scientific discovery process. In this paper, we introduce DERIVA, a framework and infrastructure for asset management in eScience and present initial results from its usage in active research use cases.\", 2016, 'e-Science', '10.1109/eScience.2016.7870883', 6, None)\n",
      "('361e9ae590d307641dd14898f0076f54ce8964f9', 'A framework for scientific workflow reproducibility in the cloud', 'Workflow is a well-established means by which to capture scientific methods in an abstract graph of interrelated processing tasks. The reproducibility of scientific workflows is therefore fundamental to reproducible e-Science. However, the ability to record all the required details so as to make a workflow fully reproducible is a long-standing problem that is very difficult to solve. In this paper, we introduce an approach that integrates system description, source control, container management and automatic deployment techniques to facilitate workflow reproducibility. We have developed a framework that leverages this integration to support workflow execution, re-execution and reproducibility in the cloud and in a personal computing environment. We demonstrate the effectiveness of our approach by examining various aspects of repeatability and reproducibility on real scientific workflows. The framework allows workflow and task images to be captured automatically, which improves not only repeatability but also runtime performance. It also gives workflows portability across different cloud environments. Finally, the framework can also track changes in the development of tasks and workflows to protect them from unintentional failures.', 2016, 'e-Science', '10.1109/eScience.2016.7870888', 16, None)\n",
      "('36ba9cf559e8622164b736744f77fa326ae039dc', 'Converting scripts into reproducible workflow research objects', 'Scientific discovery and analysis are increasingly computational and data-driven. While scripting languages, such as Python, R and Perl, are the means of choice of the majority of scientists to encode and run their data analysis, scripts are generally not amenable to reuse or reproducibility. Scripts do rarely get reused or even shared with third party scientists. We argue in this paper that the reproducibility of scripts can be promoted by converting them into workflow research objects. A workflow research object encodes a script into a production (executable) workflow that is accompanied by annotations, example datasets and provenance traces of their execution, thereby allowing third party users to understand the data analysis encoded by the original script, run the associated workflow using the same or different dataset, or even repurpose it for a different analysis. To this end, we present a methodology for converting scripts into workflow research objects in a principled manner, guided by requirements that we elicited for this purpose. The methodology exploits tools and standards that have been developed by the community, in particular YesWorkflow, Research Objects and the W3C PROV. It is showcased using a real world use case from the field of Molecular Dynamics.', 2016, 'e-Science', '10.1109/eScience.2016.7870887', 6, None)\n",
      "('391cb191e26b7e95ae0438ba187b9155bf3f1419', 'Reproducibility model for wireless sensor networks parallel simulations', 'Several wireless sensor networks (WSNs) simulations run in parallel computer architectures to improve their scalability. The main problem with this strategy is guaranteeing the reproducibility transparently to simulation users. We present a reproducible model for WSNs parallel simulations. The model uses a chunk partition strategy, in which all simulation elements are wrapped into chunks and simulated sequentially inside each chunk. We can simulate multiple chunks in parallel as often as possible by adding seed and a pseudo-random number generator in each of them, thus ensuring the same results. This model was integrated into the JSensor simulator and validated accordingly. An important aspect is that the new reproducibility model does not alter the simulationâ\\x80\\x99s semantics, and it is transparent to the developer. The results demonstrated that our model could guarantee the reproducibility of stochastic WSNs parallel simulations performed in different computer systems with different threads.', 2020, 'TJS', '10.1007/s11227-020-03298-8', 0, None)\n",
      "('3c38b65f1fb292d3b998eadb9035e496f3dffd52', 'Parallel Reproducible Summation', \"Reproducibility, i.e. getting bitwise identical floating point results from multiple runs of the same program, is a property that many users depend on either for debugging or correctness checking in many codes [10]. However, the combination of dynamic scheduling of parallel computing resources, and floating point nonassociativity, makes attaining reproducibility a challenge even for simple reduction operations like computing the sum of a vector of numbers in parallel. We propose a technique for floating point summation that is reproducible independent of the order of summation. Our technique uses Rump's algorithm for error-free vector transformation [7], and is much more efficient than using (possibly very) high precision arithmetic. Our algorithm reproducibly computes highly accurate results with an absolute error bound of n Â· 2-28 macheps maxiIviI at a cost of 7n FLOPs and a small constant amount of extra memory usage. Higher accuracies are also possible by increasing the number of error-free transformations. As long as all operations are performed in to-nearest rounding mode, results computed by the proposed algorithms are reproducible for any run on any platform. In particular, our algorithm requires the minimum number of reductions, i.e. one reduction of an array of six double precision floating point numbers per sum, and hence is well suited for massively parallel environments.\", 2015, 'TC', '10.1109/TC.2014.2345391', 34, None)\n",
      "('53e998b0b7602d97020e895a', 'Design and operation of ETA, an automated ellipsometer', 'The design and operational features are described for a computer-assisted ellipsometer (called ETA for Ellipsometric Thickness Analyzer), developed to provide reliable. real-time measurement of field-effect transistor gate insulator thickness in a manufacturing environment. ETA illuminates the sample with light of fixed polarization and uses a rotating analyzer to measure the polarization of the reflected light. Sample alignment is done automatically by ETA, so that usually no operator adjustments are required. Fourier analysis of the light transmitted by the analyzer is used to reduce noise and enhance measurement precision. In its normal mode of operation (incident light linearly polarized at 45°), ETA can measure single and double-layer films of SiO2, and Si3N4, in the thickness range of 300 to 800 Å with precision comparable to that of conventional ellipsometers. Other modes of operation, which make use of a fixed-position compensator in the incident light path, allow precise measurement of thin films (0 to 300Å) and permit use of ETA as a general-purpose ellipsometer. The typical time interval required for wafer alignment, data acquisition, analysis and recorded output of film thickness is about five seconds, and the measurement reproducibility is typically about 1Å.', 1973, 'IBMRD', '10.1147/rd.176.0472', 0, None)\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "repeatability-replicability-reproducibility-corroborability-all\n",
      "General Statistics: \n",
      " result length 3098 \n",
      "\n",
      "('0059704c451d592aba859d6e63db365f06beb3bc', 'A Nine-Valued Circuit Model for Test Generation', 'A nine-valued circuit model for test generation is introduced which takes care of multiple and repeated effects of a fault in sequential circuits. Using this model test sequences can be determined which allow multiple and repeated effects of faults on the internal state of a sequential circuit. Thus valid test sequences are derived where other known procedures, like the D-algorithm, do not find any test although one exists.', 1976, 'TC', '10.1109/TC.1976.1674663', 69, None)\n",
      "('00aeab0f74cfaeae2095deadc82f68b395304f0c', 'An Improved Search Method for Accumulator-Based Test Set Embedding', 'In this paper we present a new search method for test set embedding using an accumulator driven with an additive constant C. We formulate the problem of finding the location of a test pattern in the generated sequence in terms of a linear Diophantine equation with two variables, which is known to be solved quickly in linear time. We show that only one Diophantine equation needs to be solved per test set irrespective of its size. Next we show how to find the starting state, for a given constant C and test set T, such that the generated sequence can reproduce T with minimum length. Finally, we show that the best constant Copt (in terms of shortest test length) for the embedding of T using an accumulator of size n can be found in O(2ldrn+Fldr|T|) steps, instead of O(nldr2nldr|T|) steps of a previous approach, where F depends on the particular test set and can be significantly smaller than its worst case value of 2n-2. The value of F can also be further reduced while providing a guaranteed approximation bound of the shortest test length. Experimental results show the computational improvements.', 2009, 'TC', '10.1109/TC.2008.182', 6, None)\n",
      "('010890949ef4233a8b5965112edd35ac6d566e6c', 'Deep Facial Recognition using Tensorflow', 'Facial recognition is a tractable problem today because of the prevalence of Deep Learning implementations. Approaches for creating structured datasets from unstructured web data are more easily accessible as are GPUs that deep learning frameworks can use to learn from this data. In DARPAâ\\x80\\x99s MEMEX effort, which sought to create better search capabilities for law enforcement to scan the deep and dark web, we are interested in leveraging the Tensorflow framework to reproduce a seminal Deep Learning facial recognition model called VGG- Face. On MEMEX we desired to build the VGG-Face model and to train feature extraction for use in prioritization of leads for possible law enforcement follow-up. We describe our efforts to recreate the VGG-Face dataset, along with our efforts to create the Deep Learning network implementation for it using Tensorflow. Though other implementations of VGG-Face on Tensorflow exist, none of them fully reproduce as much of the dataset as we do today (â\\x88¼ 48% of the data still exists), nor have detailed documentation and steps for reproducing each step in the workflow. We contribute those instructions and leverage Texas Advanced Computing Centerâ\\x80\\x99s Maverick2 supercomputer to perform the work. We report experimental results on building the dataset, and training the network to achieve a 77.99% validation accuracy on the 2, 622 celebrity use case from VGG-Face. This paper can be a useful recipe in building new Tensorflow facial recognition.', 2019, 'ISC', '10.1109/DLS49591.2019.00011', 0, None)\n",
      "('020b0ee8a6cddcda2347c18b7c1efd908288ea2f', 'Software Defined Cyberinfrastructure for Data Management', 'Scientific research is data-centric, relying on the acquisition, management, movement, analysis, and sharing of data. Proficiently managing the end-to-end lifecycle of scientific data is non-trivial and comprises many time consuming and mundane tasks. While individual tasks are not prohibitive, when done repeatedly and frequently they represent a significant strain on researchers. We posit that a better approach is to automate these tasks through a Software Defined Cyberinfrastructure. We have developed R IPPLE to provide such capabilities by automating research data management activities via a programmable and event-based cyber-environment. Users specify high-level management policies, such as data movement and metadata extraction, using intuitive If-Trigger-Then-Action rules. These rules are then autonomously, and reliably, executed and managed by Ripple.', 2017, 'e-Science', '10.1109/eScience.2017.69', 1, None)\n",
      "('027fadfadd603af091e35dbcfe8ac7b70a95580d', 'On estimating the useful work distribution of parallel programs under P3T: a static performance estimator', \"In order to improve a parallel program's performance it is critical to evaluate how even the work contained in a program is distributed over all processors dedicated to the computation. Traditional work distribution analysis is commonly performed at the machine level. The disadvantage of this method is that it cannot identify whether the processors are performing useful or redundant (replicated) work. This paper describes a novel method of statically estimating the useful work distribution of distributed memory parallel programs at the program level, which carefully distinguishes between useful and redundant work. The amount of work contained in a parallel program, which correlates with the number of loop iterations to be executed by each processor, is estimated by accurately modeling loop iteration spaces, array access patterns and data distributions. A cost function deenes the useful work distribution of loops, procedures and the entire program. Lower and upper bounds of the described parameter are presented. The computational complexity of the cost function is independent of the program's problem size, statement execution and loop iteration counts. As a consequence, estimating the work distribution based on the described method is considerably faster than simulating or actually executing the program. Automatically estimating the useful work distribution is fully implemented as part of the P 3 T, which is a static parameter based performance prediction tool under the Vienna Fortran Compilation System (VFCS). The Lawrence Livermore Loops are used as a test-case to verify the approach.\", 1996, 'CCPE', '10.1002/(SICI)1096-9128(199605)8:4%3C261::AID-CPE205%3E3.0.CO;2-6', 1, None)\n",
      "('0285fc9e6578a1c66b26d5bf0089b87b5b6f10ce', 'Weather radar data processing on graphic cards', 'Weather radar operation generates data at a high rate that requires prompt processing. The operations performed on data for weather product generation are repeated in each resolution cell and thus are naturally prone to parallelization. Parallel processing using graphic cards is an emerging technology that allows for implementation of high-throughput algorithms at a low cost. In this paper, the parallel implementation of the main product of a polarimetric weather radar using GPU is presented, focusing on its optimization. A speedup exceeding 20$$\\\\times $$Ã\\x97 is obtained when compared to the serial implementation. Also processing is found to be memory bound, which results in a counter-intuitive performance improvement when the number of threads per job is reduced.', 2017, 'TJS', '10.1007/s11227-017-2166-8', 3, None)\n",
      "('043e755a2c7ae35e401314a4b93a829d7792ab3f', 'Locating Corruptions in a Replicated File in a Distributed Environment', 'When a data file is replicated at more than one site, it is of interest to detect corruption by comparing the multiple copies. In order to reduce the amount of messaging for large files, techniques based on page signatures and combined signatures have been explored. However, for 3 or more sites, the known methods assume that the number of corrupted page copies is at most â\\x8c\\x8aM/2â\\x8c\\x8b â\\x88\\x92 1, where M is the number of sites. This is a pessimistic assumption which is unrealistic. In this paper, this assumption is replaced by another assumption which is shown to be reasonable. Based on this assumption, and based on a finer model of the system, three distributed algorithms are derived, which can either improve the performance or provide more tolerance to corruptions compared to previous methods. As in some previous work, the amount of signature transmission in the algorithms varies according to the number and patterns of page copy corruptions that actually occur, and two of the algorithms achieve the optimal amount of signature transmission when no failure occurs.', 2004, 'TJS', '10.1023/A:1008066818394', 1, None)\n",
      "('0634773aa444ffe86f58b24205a7438573a5b5b3', 'Analysis of a Shared Resource MIMD Computer Organization', 'In computer architecture investigations a major interest is in the analysis of organizations which explore parallelism, and therefore, may have multiple instruction and multiple data streams (MIMD). In this paper, a MIMD structure with one shared resource is analyzed for both synchronous and asynchronous request arrivals. Although the analysis is somewhat restricted, it appears that a similar approach can be used to extend the model to more general MIMD cases. Results presented for one particular set of parameters indicate that an optimum resource replication is possible such that an optimum cost performance is achieved.', 1978, 'TC', '10.1109/TC.1978.1674953', 5, None)\n",
      "('0778db63a6485eec2f53e65e283b212561c8752f', 'Wasted dynamic power and correlation to instruction set architecture for CPU throttling', 'Reducing dynamic power consumption is one of the major design goals in modern high-performance processor design. Throttling is a mechanism that reduces dynamic power at the expense of reduced throughput. Instruction profiling can identify a set of instructions suitable for fine-grained throttling without significant performance degradation. In this paper, an Electronic Design Automation (EDA) flow was developed to process pipeline trace at an early stage to identify the bottleneck. Using the developed EDA flow, this work identifies a set of instructions suitable for fine-grained CPU throttling to reduce wasted dynamic power in RISC-V architecture. To rank higher stall causing instructions in the instruction profile, a weight-based system was introduced. It was observed that independent of the workload and type, higher stall causing instructions were repeating across all the benchmark programs. The top 10 instruction profiles for each test suite identify probable throttling clock cycles for each pipeline stage for wasted dynamic power reduction at minimal performance loss. These results are expected to enable researchers to reduce wasted dynamic power by modifying existing architecture and effectively apply throttling mechanism without significant performance degradation.', 2018, 'TJS', '10.1007/s11227-018-2637-6', 1, None)\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-or-repeatability-system\n",
      "skipped\n",
      "\n",
      "\n",
      "################################################################################ \n",
      "\n",
      "reproducibility-or-repeatability-systems\n",
      "skipped\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    print(\"#\"*80, \"\\n\")\n",
    "    test_query(q)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
