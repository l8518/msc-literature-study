{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(user=\"lvs215\",\n",
    "                                   password=\"\",\n",
    "                                   host=\"localhost\",\n",
    "                                   port=\"5432\",\n",
    "                                   database=\"lvs215\")\n",
    "start_year = 2011  # inclusive\n",
    "end_year = 2020  # inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of papers that have at least one author assigned 77.91\n"
     ]
    }
   ],
   "source": [
    "# This cell outputs how many articles we have some author data on.\n",
    "with conn.cursor() as cursor:\n",
    "    query = \"\"\"\n",
    "SELECT count(id), (SELECT COUNT(DISTINCT(id))\n",
    "    FROM author_paper_pairs, publications \n",
    "    WHERE publications.id = author_paper_pairs.paper_id\n",
    ") AS number_papers_authors\n",
    "FROM publications\n",
    "\"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    row = cursor.fetchone()\n",
    "    print(\"Amount of papers that have at least one author assigned {:.2f}\".format((row[1] / row[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of papers that have citation information 75.57%\n"
     ]
    }
   ],
   "source": [
    "# This cell outputs how many articles we have some number of citation data on.\n",
    "with conn.cursor() as cursor:\n",
    "    query = \"\"\"\n",
    "SELECT count(id), (SELECT COUNT(DISTINCT(id))\n",
    "    FROM publications \n",
    "    WHERE publications.n_citations >= 0\n",
    ") AS number_articles_citation_info\n",
    "FROM publications\n",
    "\"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    row = cursor.fetchone()\n",
    "    print(\"Amount of papers that have citation information {:.2f}%\".format((row[1] / row[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check the amount of article entries in DBLP, Semantic Scholar, and AMiner.\n",
    "def wccount(filename):\n",
    "    out = subprocess.Popen(['wc', '-l', filename],\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT\n",
    "                         ).communicate()[0]\n",
    "    return int(out.partition(b' ')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e4bed130b580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e4bed130b580>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"semantic_scholar,{},{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_semantic_scholar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_articles_semantic_scholar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DBLP: {}/{}: {:2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_dblp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_articles_dblp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_dblp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msystem_articles_dblp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aminer: {}/{}: {:2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_aminer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_articles_aminer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_aminer\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msystem_articles_aminer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Semantic Scholar: {}/{}: {:2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_semantic_scholar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_articles_semantic_scholar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_semantic_scholar\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msystem_articles_semantic_scholar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from os.path import isfile\n",
    "\n",
    "from datetime import datetime\n",
    "from venue_mapper.venue_mapper import VenueMapper\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from util import iterload_file_lines, iterload_file_lines_gzip\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    total_dblp = 0\n",
    "    total_aminer = 0\n",
    "    total_semantic_scholar = 0\n",
    "    system_articles_dblp = 0\n",
    "    system_articles_aminer = 0\n",
    "    system_articles_semantic_scholar = 0\n",
    "    venue_mapper = VenueMapper()\n",
    "\n",
    "    file_locations = \"/var/scratch/lvs215/aip_data\"\n",
    "\n",
    "    # Create a list of all the files we want to parse. Skip the compressed sources if they are still lingering around\n",
    "    for path, subdirs, files in os.walk(file_locations):\n",
    "        for name in files:\n",
    "            if isfile(os.path.join(path, name)) and not name.endswith((\"zip\", \"tar\")):\n",
    "                file_path = os.path.join(path, name)\n",
    "                if re.match(\".*dblp[\\w-]+\\.xml\", file_path):\n",
    "                    for event, element in etree.iterparse(file_path, load_dtd=True, dtd_validation=True):\n",
    "                        total_dblp += 1\n",
    "                        venue = element.find('booktitle')  # type: Optional[str]\n",
    "                        if venue is None and len(element.findall('journal')) > 0:\n",
    "                            venue = element.find('journal')\n",
    "\n",
    "                        if venue is not None and venue.text is not None:\n",
    "                            venue = str(venue.text)\n",
    "                        else:\n",
    "                            venue = None\n",
    "\n",
    "                        if venue is not None and venue_mapper.get_abbreviation(venue) is not None:\n",
    "                            system_articles_dblp += 1\n",
    "\n",
    "                elif \"s2-corpus\" in file_path:\n",
    "                    file_iterator_func = iterload_file_lines_gzip if file_path.endswith(\"gz\") else iterload_file_lines\n",
    "                    publication_iterator = file_iterator_func(file_path)\n",
    "                    for publication in publication_iterator:\n",
    "                        total_semantic_scholar += 1\n",
    "                        if publication is None: \n",
    "                            continue\n",
    "\n",
    "                        if \"venue\" not in publication:\n",
    "                            continue\n",
    "\n",
    "                        venue_string = str(publication['venue'])\n",
    "                        if len(venue_string) == 0:\n",
    "                            continue\n",
    "\n",
    "                        if venue_mapper.get_abbreviation(venue_string) is not None:\n",
    "                            system_articles_semantic_scholar += 1\n",
    "                elif \"aminer_papers\" in file_path:\n",
    "                    file_iterator_func = iterload_file_lines_gzip if file_path.endswith(\"gz\") else iterload_file_lines\n",
    "                    publication_iterator = file_iterator_func(file_path)\n",
    "                    for publication in publication_iterator:\n",
    "                        total_aminer += 1\n",
    "                        if publication is None: \n",
    "                            continue\n",
    "\n",
    "                        if 'venue' not in publication:\n",
    "                            continue\n",
    "\n",
    "                        venue_string = publication['venue']\n",
    "                        if isinstance(venue_string, dict) and \"raw\" in venue_string:\n",
    "                            venue_string = venue_string[\"raw\"]\n",
    "\n",
    "                        if venue_mapper.get_abbreviation(venue_string) is not None:\n",
    "                            system_articles_aminer += 1\n",
    "    \n",
    "    \n",
    "    date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    with open(\"aip_content_per_source_{}.csv\".format(date_time), \"w\") as file1:\n",
    "        file1.write(\"dblp,{},{}\\n\".format(total_dblp, system_articles_dblp))\n",
    "        file1.write(\"aminer,{},{}\\n\".format(total_aminer, system_articles_aminer))\n",
    "        file1.write(\"semantic_scholar,{},{}\\n\".format(total_semantic_scholar, system_articles_semantic_scholar))\n",
    "        \n",
    "    print(\"DBLP: {}/{}: {:2f}\".format(total_dblp, system_articles_dblp, total_dblp / system_articles_dblp))\n",
    "    print(\"Aminer: {}/{}: {:2f}\".format(total_aminer, system_articles_aminer, total_aminer / system_articles_aminer))\n",
    "    print(\"Semantic Scholar: {}/{}: {:2f}\".format(total_semantic_scholar, system_articles_semantic_scholar, total_semantic_scholar / system_articles_semantic_scholar))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
